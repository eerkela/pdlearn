from __future__ import annotations

import numpy as np
import pandas as pd
import pdcast
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import LabelEncoder, OneHotEncoder


# TODO: CategoricalEncoder should not one-hot encode categorical inputs that
# only have 2 levels.  It should binarize these instead.
# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html


# TODO: DummyEncoder skips automatic encoding altogether for datasets where
# this may be prohibitively expensive (i.e. 20newsgroups).


######################
####    PUBLIC    ####
######################


# class DummyEncoder(BaseEstimator, TransformerMixin):
#     """A dummy encoder that does not transform data.

#     This encoder can be used when input data is already properly encoded, and
#     therefore doesn't need to be manipulated in any way.
#     """

#     def __init__(self):
#         self.feature_names = []


class FeatureEncoder(BaseEstimator, TransformerMixin):
    """An Sklearn transformer that encodes arbitrary DataFrames for use as
    features in an AutoML model.

    Attributes
    ----------
    encoders : dict
        A dictionary mapping column names to their corresponding series
        encoder.  These encoders work on individual columns, and the
        FeatureEncoder simply calls them in succession to determine final
        output.
    feature_names : dict
        A dictionary mapping column names to their autogenerated feature names.
        If the column is categorical, this will be a list with length equal to
        the number of unique categories that were observed in the fit data.
        Otherwise, it will be the same as the column name.  These values are
        used as column labels in the encoded data.

    Notes
    -----
    When this object is fit to data, a type check is performed on each column.
    If a column is determined to contain categorical-like data (strings,
    generic python objects, or explicitly labeled as a ``CategoricalDtype``),
    then those data will be automatically `one-hot <https://en.wikipedia.org/wiki/One-hot>`_
    encoded.  Otherwise, the data will be converted into ``float64`` format and
    interpreted as a continuous variable.

    The output from this transformer may be very wide, as a new column is added
    for each level of categorical input.  As a result, it may not be the most
    memory-efficient format for training purposes.  
    """

    def __init__(self):
        self.encoders = {}  # column name -> encoder
        self.feature_names = {}  # column name -> feature name

    def fit(self, X: pd.DataFrame, y=None) -> FeatureEncoder:
        """Fit a FeatureEncoder to test data."""
        # iterate through columns to detect categorical inputs
        for col_name, col in X.items():
            # detect type of series
            col_type = pdcast.detect_type(col)

            # if series is categorical-like, use a CategoricalEncoder
            if col_type.strip().is_subtype("bool, int"):
                print(f"integer found: {col_name}, {col_type}")
                enc = NumericEncoder(as_integer=True)
            elif col_type.is_categorical or col_type.is_subtype("str, object"):
                enc = CategoricalEncoder(one_hot=True)
            else:
                enc = NumericEncoder(as_integer=False)

            # remember settings
            self.encoders[col_name] = enc.fit(col)
            self.feature_names[col_name] = enc.get_feature_names()

        return self

    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:
        """Fit a FeatureEncoder to test data and then encode it."""
        features = None

        # iterate through columns to detect categorical inputs
        for col_name, col in X.items():
            # detect type of series
            col_type = pdcast.detect_type(col)

            # if series is categorical-like, use a CategoricalEncoder
            if col_type.strip().is_subtype("bool, int"):
                enc = NumericEncoder(as_integer=True)
            elif col_type.is_categorical or col_type.is_subtype("str, object"):
                enc = CategoricalEncoder(one_hot=True)
            else:  # use a NumericEncoder
                enc = NumericEncoder(as_integer=False)

            # remember settings
            result = enc.fit_transform(col)
            features = result if features is None else features.join(result)
            self.encoders[col_name] = enc
            self.feature_names[col_name] = enc.get_feature_names()

        return features

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """Encode test data."""
        if not self.encoders:
            raise RuntimeError(
                f"Cannot encode features: FeatureEncoder has not yet been fit"
            )

        # reorder columns in X to match encoders
        X = reorder_columns(X, list(self.encoders))

        # build encoded result
        result = None
        for col_name, col in X.items():
            encoded = self.encoders[col_name].transform(col)
            result = encoded if result is None else result.join(encoded)
        return result

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """Decode test data."""
        if not self.encoders:
            raise RuntimeError(
                "Cannot decode features: FeatureEncoder has not yet been fit"
            )

        # empty DataFrame with same index
        reconstructed = pd.DataFrame(index=X.index)

        # iterate through decoders and invert related features
        for col_name, features in self.feature_names.items():
            features = X.loc[:, features]
            result = self.encoders[col_name].inverse_transform(features)
            reconstructed[col_name] = result

        return reconstructed

    def get_feature_names(self) -> np.array:
        """Get output feature names for transformation."""
        names = []
        for features in self.feature_names.values():
            names.extend(features)
        return np.array(names, dtype=object)


class TargetEncoder(BaseEstimator, TransformerMixin):
    """An Sklearn transformer that encodes arbitrary DataFrames for use as
    targets in an AutoML model.

    Parameters
    ----------
    classify : bool, default False
        Specifies whether to interpret columns as targets for classification
        (True) or regression (False).  If ``classify=True``, each value will
        be encoded as an integer label in the output.  Otherwise, they will be
        cast to ``float64``.

    Attributes
    ----------
    encoders : dict
        A dictionary mapping column names to their corresponding series
        encoder.  These encoders work on individual columns, and the
        TargetEncoder simply calls them in succession to determine final
        output.
    feature_names : dict
        A dictionary mapping column names to their autogenerated feature names.
        If a column is categorical, this will be a list with length equal to
        the number of unique categories that were observed in the fit data.
        Otherwise, it will be the same as the column name.  These values are
        used as column labels in expanded probability data.
    """

    def __init__(self, classify: bool = False):
        self.classify = classify
        self.encoders: dict[str, BaseEstimator] = {}
        self.feature_names: dict[str, list[str]] = {}

    def fit(self, X: pd.DataFrame, y=None) -> FeatureEncoder:
        """Fit a TargetEncoder to test data."""
        # iterate through columns to detect categorical inputs
        for col_name, col in X.items():
            # if TargetEncoder is meant for classification, don't use one hot
            if self.classify:
                enc = CategoricalEncoder(one_hot=False)
            elif pdcast.detect_type(col).strip().is_subtype("bool, int"):
                enc = NumericEncoder(as_integer=True)
            else:
                enc = NumericEncoder(as_integer=False)

            # remember settings
            self.encoders[col_name] = enc.fit(col)
            self.feature_names[col_name] = enc.get_feature_names()

        return self

    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:
        """Fit a FeatureEncoder to test data and then encode it."""
        target = None

        # iterate through columns to detect categorical inputs
        for col_name, col in X.items():
            # if TargetEncoder is meant for classification, don't use one hot
            if self.classify:
                enc = CategoricalEncoder(one_hot=False)
            elif pdcast.detect_type(col).strip().is_subtype("bool, int"):
                enc = NumericEncoder(as_integer=True)
            else:
                enc = NumericEncoder(as_integer=False)

            # remember settings
            result = enc.fit_transform(col)
            target = result if target is None else target.join(result)
            self.encoders[col_name] = enc
            self.feature_names[col_name] = enc.get_feature_names()

        return target

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """Encode test data."""
        if not self.encoders:
            raise RuntimeError(
                "Cannot encode targets: TargetEncoder has not yet been fit"
            )

        # reorder columns in X to match encoders
        X = reorder_columns(X, list(self.encoders))

        # build encoded result
        result = None
        for col_name, col in X.items():
            encoded = self.encoders[col_name].transform(col)
            result = encoded if result is None else result.join(encoded)
        return result

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """Decode test data."""
        if not self.encoders:
            raise RuntimeError(
                "Cannot decode targets: TargetEncoder has not yet been fit"
            )

        # empty DataFrame with same index
        reconstructed = pd.DataFrame(index=X.index)

        # iterate through decoders and invert related features
        for col_name, encoder in self.encoders.items():
            target = X.loc[:, [col_name]]
            reconstructed[col_name] = encoder.inverse_transform(target)

        return reconstructed

    def get_feature_names(self) -> np.ndarray:
        """Get output feature names for transformation."""
        names = []
        for features in self.feature_names.values():
            names.extend(features)
        return np.array(names, dtype=object)


#######################
####    PRIVATE    ####
#######################


class CategoricalEncoder(BaseEstimator, TransformerMixin):
    """An sklearn encoder that converts categorical inputs of any kind into a
    corresponding one-hot matrix for use in model fits.
    """

    def __init__(self, one_hot: bool = True):
        self.use_one_hot = one_hot
        self.labels = LabelEncoder()
        self.one_hot = OneHotEncoder(sparse=False, dtype=np.int8)
        self.col_name = None
        self.dtype = None

    def fit(self, X: pd.Series, y=None) -> CategoricalEncoder:
        """Fit a CategoricalEncoder to test data."""
        # remember series attributes
        self.col_name = X.name
        self.dtype = pdcast.detect_type(X)

        # pass through LabelEncoder, (optionally) OneHotEncoder
        if self.use_one_hot:
            result = self.labels.fit_transform(X)
            self.one_hot.fit(np.atleast_2d(result).T)
        else:
            self.labels.fit(X)

        return self

    def fit_transform(self, X: pd.Series, y=None) -> pd.DataFrame:
        """Fit a CategoricalEncoder to test data and then encode it."""
        # remember series attributes
        self.col_name = X.name
        self.dtype = pdcast.detect_type(X)

        # pass through LabelEncoder, (optionally) OneHotEncoder
        result = self.labels.fit_transform(X)
        if self.use_one_hot:
            result = self.one_hot.fit_transform(np.atleast_2d(result).T)

        return pd.DataFrame(
            result,
            columns=self.get_feature_names(),
            index=X.index
        )

    def transform(self, X: pd.Series) -> pd.DataFrame:
        """Transform test data using categorical one-hot encoding."""
        result = self.labels.transform(X)
        if self.use_one_hot:
            result = self.one_hot.transform(np.atleast_2d(result).T)
        return pd.DataFrame(
            result,
            columns=self.get_feature_names(),
            index=X.index
        )

    def inverse_transform(self, X: pd.DataFrame) -> pd.Series:
        """Convert encoded data back to its original representation."""
        if self.use_one_hot:
            result = self.one_hot.inverse_transform(X)
        else:
            result = X.to_numpy()
        return pd.Series(
            self.labels.inverse_transform(result.ravel()),
            dtype=self.dtype.dtype,
            name=self.col_name,
            index=X.index
        )

    def get_feature_names(self) -> np.ndarray:
        """Get output feature names for transformation."""
        if self.use_one_hot:
            names = [f"{self.col_name}.{str(x)}" for x in self.labels.classes_]
        else:
            names = [self.col_name]
        return np.array(names, dtype=object)


class NumericEncoder(BaseEstimator, TransformerMixin):
    """An sklearn encoder that converts numeric inputs into a continuous float
    representation for use in model fits.
    """

    def __init__(self, as_integer: bool = False, downcast: bool = True):
        self.as_integer = as_integer
        self.downcast = downcast
        self.col_name = None
        self.dtype = None

    def fit(self, X: pd.Series, y=None) -> NumericEncoder:
        """Fit a NumericEncoder to test data."""
        self.col_name = X.name
        self.dtype = pdcast.detect_type(X)
        return self

    def fit_transform(self, X: pd.Series) -> pd.DataFrame:
        """Fit a NumericEncoder to test data and then encode it."""
        self.col_name = X.name
        self.dtype = pdcast.detect_type(X)
        if self.as_integer:
            result = pdcast.cast(X, "int", downcast=self.downcast)
        else:
            result = pdcast.cast(X, "float", downcast=self.downcast)
        return pd.DataFrame({self.col_name: result}, index=X.index)

    def transform(self, X: pd.Series) -> pd.DataFrame:
        """Transform test data, converting it into float representation."""
        if self.as_integer:
            result = pdcast.cast(X, "int", downcast=self.downcast)
        else:
            result = pdcast.cast(X, "float", downcast=self.downcast)
        return pd.DataFrame({self.col_name: result}, index=X.index)

    def inverse_transform(self, X: pd.DataFrame) -> pd.Series:
        """Convert encoded data back to its original representation."""
        if X.shape[1] != 1:
            raise ValueError(f"Input data must have only one column:\n{X}")

        result = pdcast.cast(X.iloc[:, 0], self.dtype, tol=np.inf)
        result.name = self.col_name
        return result

    def get_feature_names(self) -> np.array:
        """Get output feature names for transformation."""
        return np.array([self.col_name], dtype=object)


def reorder_columns(df: pd.DataFrame, order: list) -> pd.DataFrame:
    """Reorder columns in ``df`` to match the given order."""
    cols = df.columns.tolist()

    # check that every required column is present in df
    missing = [x for x in order if x not in cols]
    if missing:
        raise KeyError(f"DataFrame is missing required columns: {missing}")

    # check that every column in df is present in order
    extra = [x for x in cols if x not in order]
    if extra:
        raise KeyError(f"DataFrame has extra columns: {extra}")

    # rearrange columns to match expected order
    cols = sorted(cols, key=lambda x: order.index(x))
    return df[cols]
